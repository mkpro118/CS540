{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a870a62-4796-44b9-9a1a-5165c59f048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "try:\n",
    "    from typing import Self\n",
    "except ImportError:\n",
    "    from typing_extensions import Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831a995b-7b72-4be1-b1ef-299d45f8c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_str(arr):\n",
    "    if arr.ndim == 1:\n",
    "        return ','.join(map(str, np.around(arr, 4).tolist()))\n",
    "    elif arr.ndim == 2:\n",
    "        return '\\n'.join((','.join(map(str, x)) for x in np.around(arr, 4).tolist()))\n",
    "    else:\n",
    "        raise ValueError('Only 1 and 2-d arrays suppported')\n",
    "\n",
    "def write(file: str, data: Any):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(data if isinstance(data, str) else str(data))\n",
    "\n",
    "def rescale(x: np.ndarray) -> np.ndarray:\n",
    "    min_ = np.min(x)\n",
    "    range_ = np.ptp(x)\n",
    "    return (x - min_) / range_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b718a53-3fb8-4a5a-86df-f4e1088fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('time_series_covid19_deaths_US_raw.csv')\n",
    "raw_data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a133db-017a-4d05-ad5d-7a5dfdea9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_data = np.cumsum(raw_data, axis=0)\n",
    "data = np.diff(cum_data)\n",
    "\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c78a7e1-ca01-4372-a964-71e0e43853a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wisc_cal_data = cum_data[(4, 48), :]\n",
    "# q1_str = array_to_str(wisc_cal_data)\n",
    "# write('q1.txt', q1_str)\n",
    "\n",
    "# del cum_data\n",
    "# del q1_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0776207-950f-405a-97e1-61be003f86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2_data = data[(4, 48), :] # 4-> cal, 48 -> wisco\n",
    "# q2_str = array_to_str(q2_data)\n",
    "# write('q2.txt', q2_str)\n",
    "\n",
    "# del q2_data\n",
    "# del q2_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e5307f-105e-41b8-926b-1598e787149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write('q3.txt', '''Mean of the time-differenced data\n",
    "# Standard deviation of the time-differenced data\n",
    "# Median of the time-differenced data\n",
    "# Linear trend coefficient of the data\n",
    "# Auto-correlation of the data''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf249da-68f6-43ea-aa31-9b87aae921de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data, axis=1)\n",
    "mean = rescale(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eef45d3-b077-4671-b23c-c37f3deedd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(data, axis=1)\n",
    "std = rescale(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3427de-0d2f-4723-8c6a-8edb51e2db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(data, axis=1)\n",
    "median = rescale(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b596765-6524-4eca-8a2f-b4aa12303de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc = np.array([sum((x[i] - mean[idx]) * (i + 1 - (half_length := ((data.shape[-1] + 1) / 2))) for i in range(len(x))) for idx, x in enumerate(data.tolist())]) / sum((i+1 - half_length) ** 2 for i in range(data.shape[-1]))\n",
    "ltc = rescale(ltc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b14d1ecb-1370-4401-8602-6e57de68e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = np.array([ sum((x[i] * x[i-1] for i in range(1, len(x)))) / np.sum(np.power(x, 2)) for x in (data - mean.reshape(50, -1)).tolist()])\n",
    "ac = rescale(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98593ab8-e689-499d-9236-331944c16b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametric_data = np.empty((50, 5))\n",
    "parametric_data[:, 0] = mean\n",
    "parametric_data[:, 1] = std\n",
    "parametric_data[:, 2] = median\n",
    "parametric_data[:, 3] = ltc\n",
    "parametric_data[:, 4] = ac\n",
    "\n",
    "# q4_str = array_to_str(parametric_data)\n",
    "# write('q4.txt', q4_str)\n",
    "# del q4_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd919c80-3ece-4075-88de-8333eb53a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClustering:\n",
    "    class Clustering:\n",
    "        @staticmethod\n",
    "        def _compute_distances(X: np.ndarray) -> np.ndarray:\n",
    "            n = X.shape[0]\n",
    "            distance_matrix = np.zeros((n, n))\n",
    "        \n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    distance_matrix[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "        \n",
    "            distance_matrix += distance_matrix.T\n",
    "        \n",
    "            return distance_matrix\n",
    "        \n",
    "        @staticmethod\n",
    "        def _single(distance_matrix: np.ndarray, num_clusters: int) -> list:\n",
    "            num_points = distance_matrix.shape[0]\n",
    "            clusters = [[i] for i in range(num_points)]\n",
    "        \n",
    "            while len(clusters) > num_clusters:\n",
    "                min_distance = np.inf\n",
    "                merge_index1, merge_index2 = None, None\n",
    "        \n",
    "                for i in range(len(clusters)):\n",
    "                    for j in range(i + 1, len(clusters)):\n",
    "                        cluster1 = clusters[i]\n",
    "                        cluster2 = clusters[j]\n",
    "        \n",
    "                        distance = np.min(distance_matrix[np.ix_(cluster1, cluster2)])\n",
    "        \n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            merge_index1, merge_index2 = i, j\n",
    "        \n",
    "                # Merge the two closest clusters\n",
    "                clusters[merge_index1].extend(clusters[merge_index2])\n",
    "                del clusters[merge_index2]\n",
    "        \n",
    "                combined_cluster = clusters[merge_index1]\n",
    "                for i in range(distance_matrix.shape[0]):\n",
    "                    if i in combined_cluster:\n",
    "                        continue\n",
    "                    distance = np.min(distance_matrix[np.ix_(combined_cluster, [i])])\n",
    "                    distance_matrix[i, combined_cluster] = distance\n",
    "                    distance_matrix[combined_cluster, i] = distance\n",
    "        \n",
    "            return tuple(map(tuple, clusters))\n",
    "    \n",
    "        @staticmethod\n",
    "        def _complete(distance_matrix: np.ndarray, n_clusters: int) -> list:\n",
    "            num_points = distance_matrix.shape[0]\n",
    "            clusters = [[i] for i in range(num_points)]\n",
    "        \n",
    "            while len(clusters) > n_clusters:\n",
    "                min_max_distance = np.inf\n",
    "                merge_index1, merge_index2 = None, None\n",
    "        \n",
    "                for i in range(len(clusters)):\n",
    "                    for j in range(i + 1, len(clusters)):\n",
    "                        cluster1 = clusters[i]\n",
    "                        cluster2 = clusters[j]\n",
    "        \n",
    "                        max_distance = np.max(distance_matrix[np.ix_(cluster1, cluster2)])\n",
    "        \n",
    "                        if max_distance < min_max_distance:\n",
    "                            min_max_distance = max_distance\n",
    "                            merge_index1, merge_index2 = i, j\n",
    "        \n",
    "                clusters[merge_index1].extend(clusters[merge_index2])\n",
    "                del clusters[merge_index2]\n",
    "        \n",
    "                combined_cluster = clusters[merge_index1]\n",
    "                for i in range(distance_matrix.shape[0]):\n",
    "                    if i in combined_cluster:\n",
    "                        continue\n",
    "                    distance = np.max(distance_matrix[np.ix_(combined_cluster, [i])])\n",
    "                    distance_matrix[i, combined_cluster] = distance\n",
    "                    distance_matrix[combined_cluster, i] = distance\n",
    "        \n",
    "            return clusters\n",
    "        \n",
    "    _METHODS = {\n",
    "        'single': Clustering._single,\n",
    "        'complete': Clustering._complete,\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_clusters: int = 2, linkage: str = 'single'):\n",
    "        self._n_clusters = n_clusters\n",
    "        \n",
    "        assert HierarchicalClustering._is_linkage_valid(linkage), (\n",
    "            f'Invalid Linkage, valid linkages are \\'single\\' and '\n",
    "            f'\\'complete\\', found {linkage!r}'\n",
    "        )\n",
    "        \n",
    "        self._linkage = linkage\n",
    "\n",
    "    def _build(self, X: np.ndarray):\n",
    "        distance_matrix = HierarchicalClustering.Clustering._compute_distances(X)\n",
    "        cluster_func = HierarchicalClustering._METHODS.get(self._linkage)\n",
    "        if cluster_func is None:\n",
    "            raise ValueError(\n",
    "                f'Invalid Linkage, the Hierarchical Clustering instance '\n",
    "                f'may have been tampered with. found {self.linkage!r}'\n",
    "            )\n",
    "\n",
    "        self._clusters = cluster_func(distance_matrix, self._n_clusters)\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> Self:\n",
    "        self._build(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def clusters(self):\n",
    "        return self._clusters\n",
    "\n",
    "    @clusters.setter\n",
    "    def clusters(self, value):\n",
    "        raise ValueError('The `clusters` attribute should not be modified externally!')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_linkage_valid(linkage: str) -> bool:\n",
    "        return linkage in HierarchicalClustering._METHODS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990d2115-78bb-472f-8494-7c100e1b0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slhc = HierarchicalClustering(n_clusters = 5)\n",
    "slhc = slhc.fit(parametric_data)\n",
    "\n",
    "idxs = np.arange(parametric_data.shape[0])\n",
    "\n",
    "for cluster_idx, cluster in enumerate(slhc.clusters):\n",
    "    for item in cluster:\n",
    "        idxs[item] = cluster_idx\n",
    "\n",
    "# q5_str = array_to_str(idxs)\n",
    "# write('q5.txt', q5_str)\n",
    "# del q5_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1464e1fd-b763-4c1d-9d6f-97ad3f59af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "clhc = HierarchicalClustering(n_clusters = 5, linkage='complete')\n",
    "clhc = clhc.fit(parametric_data)\n",
    "\n",
    "idxs = np.arange(parametric_data.shape[0])\n",
    "\n",
    "for cluster_idx, cluster in enumerate(clhc.clusters):\n",
    "    for item in cluster:\n",
    "        idxs[item] = cluster_idx\n",
    "\n",
    "# q6_str = array_to_str(idxs)\n",
    "# write('q6.txt', q6_str)\n",
    "# del q6_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4774cf37-17e2-425a-ad9e-cf364b24a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    class Clustering:\n",
    "        @staticmethod\n",
    "        def assign_points(centroids: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "            distances = KMeans.Clustering.compute_distances(centroids, X)\n",
    "\n",
    "            return np.argmin(distances, axis=1)\n",
    "        \n",
    "        @staticmethod\n",
    "        def choose_centroids(X: np.ndarray, k: int = 2) -> np.ndarray:\n",
    "            return np.random.default_rng().choice(\n",
    "                X,\n",
    "                size=k,\n",
    "                replace=False,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "        @staticmethod\n",
    "        def compute_centroids(X: np.ndarray, assignments: np.ndarray, k: int) -> np.ndarray:\n",
    "            centroids = np.empty((k, X.shape[1]))\n",
    "            for i in range(k):\n",
    "                idxs = assignments == i\n",
    "                centroids[i] = np.mean(X[idxs], axis=0)\n",
    "            return centroids\n",
    "        \n",
    "        @staticmethod\n",
    "        def compute_distances(centroids: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "            return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "    \n",
    "    def __init__(self, k: int = 2, max_iters: int = 100, tol: float = 1e-5, ):\n",
    "        self._k = k\n",
    "        self._max_iters = max_iters\n",
    "        self._tol = abs(tol)\n",
    "    \n",
    "    def _build(self, X: np.ndarray):\n",
    "        self._centroids = KMeans.Clustering.choose_centroids(X, self.k)\n",
    "\n",
    "        old_centroids = np.empty_like(self._centroids)\n",
    "        for _ in range(self._max_iters):\n",
    "            np.copyto(old_centroids, self._centroids)\n",
    "            \n",
    "            assignments = KMeans.Clustering.assign_points(self._centroids, X)\n",
    "\n",
    "            self._centroids = KMeans.Clustering.compute_centroids(X, assignments, self.k)\n",
    "\n",
    "            if np.all(np.abs(self.centroids - old_centroids) < self._tol):\n",
    "                self._converged_in = _\n",
    "                break\n",
    "    \n",
    "    def fit(self, X: np.ndarray) -> Self:\n",
    "        self._build(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return KMeans.Clustering.assign_points(self._centroids, X)\n",
    "\n",
    "    def distortion(self, X: np.ndarray) -> np.ndarray:\n",
    "        assignments = self.predict(X)\n",
    "        _distortion = 0\n",
    "        \n",
    "        for i in range(centroids.shape[0]):\n",
    "            idxs = assignments == i\n",
    "            _distortion += np.sum((X[idxs] - self._centroids[i]) ** 2)\n",
    "    \n",
    "        return _distortion\n",
    "    \n",
    "    @property\n",
    "    def centroids(self):\n",
    "        return self._centroids\n",
    "    \n",
    "    @centroids.setter\n",
    "    def centroids(self):\n",
    "        raise ValueError('The `centroids` attribute should not be modified externally!')\n",
    "    \n",
    "    @property\n",
    "    def k(self):\n",
    "        return self._k\n",
    "    \n",
    "    @k.setter\n",
    "    def k(self):\n",
    "        raise ValueError('The `k` attribute should not be modified externally!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bab7950-d679-4c08-8aea-b4d32db72c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans at 0x1813bf493d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(k=5)\n",
    "kmeans.fit(parametric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40637d78-d86b-4a4d-9b16-006f04c3b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = kmeans.predict(parametric_data)\n",
    "print(predictions)\n",
    "\n",
    "q7_str = array_to_str(predictions)\n",
    "write('q7.txt', q7_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47cad3f5-a6c2-4002-9e7d-44049076c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47093786 0.47923711 0.49213724 0.58130065 0.82953252]\n",
      " [0.24875333 0.27621173 0.2607057  0.77092614 0.73894416]\n",
      " [0.90942921 0.91098634 0.91746852 0.05107117 0.98803245]\n",
      " [0.67874628 0.67784625 0.68977841 0.25297739 0.9349062 ]\n",
      " [0.01910688 0.0263451  0.01608292 0.98534334 0.09319708]]\n"
     ]
    }
   ],
   "source": [
    "centroids = kmeans.centroids\n",
    "print(centroids)\n",
    "\n",
    "q8_str = array_to_str(centroids)\n",
    "write('q8.txt', q8_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f564a1d-1337-4a86-8709-4dd5de1e69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0194499329842523\n"
     ]
    }
   ],
   "source": [
    "distortion = kmeans.distortion(parametric_data)\n",
    "print(distortion)\n",
    "\n",
    "q9_str = str(np.around(distortion, 4))\n",
    "write('q9.txt', q9_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec055bec-b7d7-420d-a305-98a33311f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans.labels_\n",
      "[2 2 2 2 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "kmeans.cluster_centers_\n",
      "[[0.3841545  0.4013964  0.40084187 0.65194206 0.78788545]\n",
      " [0.85667248 0.85702788 0.86474466 0.09064934 0.97840011]\n",
      " [0.01910688 0.0263451  0.01608292 0.98534334 0.09319708]\n",
      " [0.54569964 0.54870142 0.56709338 0.48836738 0.86210152]\n",
      " [0.21272193 0.24230699 0.22399929 0.80436664 0.73502808]]\n",
      "kmeans.inertia_\n",
      "1.2250582936983982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, n_init='auto')\n",
    "kmeans.fit(parametric_data)\n",
    "\n",
    "print('kmeans.labels_', kmeans.labels_, sep='\\n')\n",
    "print('kmeans.cluster_centers_', kmeans.cluster_centers_, sep='\\n')\n",
    "print('kmeans.inertia_', kmeans.inertia_, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b95f5751-be36-4296-8e32-9fee35f2296a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2251"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(kmeans.inertia_, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc9785-5020-4da8-b0dd-bb7b74a30ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
